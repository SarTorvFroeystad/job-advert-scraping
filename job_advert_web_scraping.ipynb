{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping a Job Advert to Map Keyword Frequency\n",
    "\n",
    "This project scrapes text from an online job advertisement and analyzes the frequency of keywords within the ad. The goal is to identify the most emphasized skills and qualifications, helping job seekers tailor their resumes and applications to specific job postings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <b> BeautifulSoup and Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# The URL of the job advertisement (this one is fake for preview purposes)\n",
    "url = 'https://www.techjobs.com/job/fulltime/ad.html?jobcode=123456'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Check the Status of the Webpage\n",
    "\n",
    "Before scraping the content, we need to ensure that the webpage is accessible by checking the HTTP status code. A status code of 200 indicates the page is available, while a 404 error means the page is not found.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status code: 200\n",
      "Full of soup! Feel free to retrieve the soup of choice.\n"
     ]
    }
   ],
   "source": [
    "def CheckSoupStatus(url):\n",
    "    \n",
    "    \"\"\" \n",
    "    Check the HTTP status code of the URL response and print a message \n",
    "    indicating whether the request was successful.\n",
    "    (200 for OK, 404 for Not Found).\n",
    "    \"\"\"\n",
    "\n",
    "    # Send a HTTP GET request to the specified URL\n",
    "    webpage = requests.get(url)\n",
    "    print(f'Status code: {webpage.status_code}')\n",
    "\n",
    "    # Provide a message/feedback based on the status code\n",
    "    if webpage.status_code == 200:\n",
    "        print('Full of soup! Feel free to retrieve the soup of choice.')\n",
    "    elif webpage.status_code == 404:\n",
    "        print('Server Error. No soup available at the moment.')\n",
    "    else:\n",
    "        print('Oh no. Something is not working.')\n",
    "\n",
    "# Check the status of the webpage\n",
    "CheckSoupStatus(url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Scrape and Parse the Webpage Content\n",
    "\n",
    "Next, we'll send a request to the URL and parse the HTML content of the page using BeautifulSoup. We'll then extract the title of the webpage and inspect the HTML structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requests.get() returns a response object, \n",
    "# which contains the server's response to the request. \n",
    "# This object includes attributes like .text (the content of the response) \n",
    "# and .status_code (the HTTP status code).\n",
    "\n",
    "# Send an HTTP GET request to the URL and get the page content\n",
    "page = requests.get(url)\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(page.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Below is the fake preview, loaded from a local HTML file. We'll act as if we just read this from a website, as shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Software Engineer Position | TechJobs.com'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_html_file = 'fictious_job_advertisement.html'\n",
    "\n",
    "# Read the HTML content from the file\n",
    "with open(fake_html_file, 'r', encoding='utf-8') as file:\n",
    "    fake_html_content = file.read()\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "fake_soup = BeautifulSoup(fake_html_content, 'html.parser')\n",
    "\n",
    "# Prints the title of the webpage as a string\n",
    "fake_soup.title.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      " <head>\n",
      "  <meta charset=\"utf-8\"/>\n",
      "  <meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n",
      "  <meta content=\"telephone=no\" name=\"format-detection\"/>\n",
      "  <title>\n",
      "   Software Engineer Position | TechJobs.com\n",
      "  </title>\n",
      "  <meta content=\"Join our innovative team as a Software Engineer, leading the development of groundbreaking applications.\" name=\"description\"/>\n",
      "  <meta content=\"Software Engineer Position | TechJobs.com\" property=\"og:title\"/>\n",
      "  <meta content=\"https://www.techjobs.com/job/fulltime/ad.html?jobcode=123456\" property=\"og:url\"/>\n",
      "  <meta content=\"Join our innovative team as a Software Engineer, leading the development of groundbreaking applications.\" property=\"og:description\"/>\n",
      " </head>\n",
      " <body>\n",
      "  <div id=\"job-advertisement\">\n",
      "   <div class=\"job-title\">\n",
      "    Software Engineer Position\n",
      "   </div>\n",
      "   <div class=\"company-name\">\n",
      "    TechJobs.com\n",
      "   </div>\n",
      "   <div class=\"job-location\">\n",
      "    San Francisco, CA\n",
      "   </div>\n",
      "   <div class=\"job-descript\n"
     ]
    }
   ],
   "source": [
    "# Prettify makes the HTML code look pretty (readable) by adding hierarchy\n",
    "# Get the prettified HTML content\n",
    "pretty_html = fake_soup.prettify()\n",
    "\n",
    "# Print only the first 1000 characters\n",
    "print(pretty_html[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Extract the Job Advertisement Text\n",
    "\n",
    "We will search for the specific `div` tag containing the job advertisement text. Once found, we will extract and clean the text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'div' parameter finds all the div tags within the HTML code\n",
    "fake_soup.find_all('div', class_='job-description');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We are looking for a passionate Software Engineer to design, develop, and implement new features for our web applications. The ideal candidate has a solid background in software engineering, experienc'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using find instead of find_all to extract text\n",
    "job_ad = fake_soup.find('div', class_='job-description').text.strip()\n",
    "\n",
    "# Showing a snippet of the string\n",
    "job_ad[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Process the Text Data\n",
    "\n",
    "We'll split the job ad text into individual words, remove any empty entries, and convert all words to lowercase to standardize them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We',\n",
       " 'are',\n",
       " 'looking',\n",
       " 'for',\n",
       " 'a',\n",
       " 'passionate',\n",
       " 'Software',\n",
       " 'Engineer',\n",
       " 'to',\n",
       " 'design',\n",
       " '',\n",
       " 'develop',\n",
       " '',\n",
       " 'and',\n",
       " 'implement']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Define multiple separators using regex pattern, including a way to parse camelCase and PascalCase strings\n",
    "pattern = r'(?<=[a-z])(?=[A-Z])|[;,/.() ]'\n",
    "\n",
    "# Split the job ad text based on the defined pattern\n",
    "word_list = re.split(pattern, job_ad)\n",
    "\n",
    "# Showing a snippet of the list\n",
    "word_list[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['we',\n",
       " 'are',\n",
       " 'looking',\n",
       " 'for',\n",
       " 'a',\n",
       " 'passionate',\n",
       " 'software',\n",
       " 'engineer',\n",
       " 'to',\n",
       " 'design',\n",
       " 'develop',\n",
       " 'and',\n",
       " 'implement',\n",
       " 'new',\n",
       " 'features']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = []\n",
    "\n",
    "# Process the word list: remove empty list entries and standardize to lowercase\n",
    "for word in word_list:\n",
    "    if word == '':\n",
    "        continue\n",
    "    else:\n",
    "        words.append(word.lower())\n",
    "\n",
    "# Showing a snippet of the list\n",
    "words[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Analyze Keyword Frequency\n",
    "\n",
    "Using Pandas, we'll count the frequency of each word in the job ad and store the results in a DataFrame. Finally, we'll save the word frequency data to a CSV file for further analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Count\n",
      "and              13\n",
      "in                5\n",
      "software          4\n",
      "to                3\n",
      "with              3\n",
      "web               3\n",
      "a                 3\n",
      "for               3\n",
      "code              2\n",
      "technologies      2\n",
      "the               2\n",
      "of                2\n",
      "python            2\n",
      "script            2\n",
      "experience        2\n"
     ]
    }
   ],
   "source": [
    "# Create a Pandas Series from the word list and count the frequency of each word\n",
    "series = pd.Series(words).value_counts()\n",
    "\n",
    "# Create a DataFrame with the word counts\n",
    "df = pd.DataFrame(index=series.index)\n",
    "df['Count'] = series\n",
    "\n",
    "# Showing the top 15 words\n",
    "print(df.head(15))\n",
    "\n",
    "# Save the word frequency DataFrame to a CSV file\n",
    "df.to_csv('word_list')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
